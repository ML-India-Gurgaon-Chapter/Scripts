{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an IPython notebook. Each cell has code which can be run independently from what is run in other cells. Once a cell is\n",
    "run the variables etc are kept and saved and not destroyed from the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 1\n",
    "\n",
    "# now go back and the run the first cell. \n",
    "# To run a cell, click inside the cell and press Shift + Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 1 - Basic visualisations and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT #1 - Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_csv.csv')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## To get help related to any Python method, module etc. Write the module or methods name on a new line followed by a ? and\n",
    "## execute.\n",
    "df?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## To know about methods in a cell. Write df. and press Tab. \n",
    "## It was show the list of methods inside df.\n",
    "## To get help for the method, type df.method_name?\n",
    "\n",
    "df.update?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## What all columns got loaded?\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## For now I am going to ignore Computer Programming, Electrical and etc scores as they are any reflected in the domain scores.\n",
    "## Also removing DOJ etc.\n",
    "## Removing ID\n",
    "## Removing college city and job city for basic model.\n",
    "\n",
    "basic_cols = [u'Salary', u'Gender', u'10percentage', u'10board', \n",
    "          u'12graduation', u'12percentage', u'12board', u'CollegeID', \n",
    "          u'CollegeTier', u'Degree', u'Specialization', u'collegeGPA', u'CollegeCityTier', u'CollegeState', \n",
    "          u'GraduationYear', u'English', u'Logical', u'Quant', u'Domain', u'conscientiousness', \n",
    "          u'agreeableness', u'extraversion', u'nueroticism', u'openess_to_experience', 'Designation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = df[basic_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT #2 - Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Analysis of the numerical columns.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT #3 - Inspecting further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Lets see the top 5 values in each columns - this will help get a feel of the non-numeric columns too like city etc.\n",
    "## Which are are non-numerical columns? All columns minus numerical columns\n",
    "non_num_cols = list(set(data.columns) - set(data._get_numeric_data().columns))\n",
    "\n",
    "for col in non_num_cols:\n",
    "    print col,'has unique values:' , data[col].unique().shape[0]\n",
    "    print data[col].value_counts().iloc[:5]\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Cleaning CGPA \n",
    "data.collegeGPA.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.collegeGPA.hist(bins=10)\n",
    "plt.title('College GPA - Histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can notice there a few people who are below ten. It seems like an anamoly. There aren't a lot of students in betweem that\n",
    "# bar and the 40 to 60 bar. It could be that the dataset has CGPA which also belong the the 0-10 scale and students didn't \n",
    "# scale it to 100. Different of scales has an issue as a the model would think a 9.98 CGPA is extremely bad in the current\n",
    "# scale but it possibly was a good cgpa. We will scale the grades below the 10 scale to a 100 scale.\n",
    "\n",
    "data.loc[data.collegeGPA <=10, 'collegeGPA'] = data.collegeGPA*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.collegeGPA.hist()\n",
    "plt.xlim(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.Domain.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Setting the data which was -1 as null object.\n",
    "for col in df.columns:\n",
    "    df.loc[df[col] == -1, col] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## So we want to predict Salary\n",
    "## But, how is the salary distibuted\n",
    "\n",
    "data.Salary.hist(xrot =90, bins = 100)\n",
    "plt.title('Salary - Histogram')\n",
    "\n",
    "## You can notice longtail. What is longtail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How does 12 board look like?\n",
    "data['12board'].value_counts().plot(kind='bar', figsize=(16,5))\n",
    "## 12 board follows a long tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How does 12 board look like for top 30 values?\n",
    "data['12board'].value_counts()[:30].plot(kind='bar', figsize=(16,5))\n",
    "## You can see the main primary boards have peaks and then there is a long tail of boards which are not always unique.\n",
    "## eg. up, upboard, up board and uttar pradesh board all mean the same thing but are represented differently.\n",
    "## We could have cleaned this data using some scripts and added this column to our analysis but for now we will not use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How do English scores of candidates look?\n",
    "data.English.hist(bins=20)\n",
    "plt.title('English - Histogram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## But how does English affect Salary?\n",
    "## What scores of English get what kinds of Salaries?\n",
    "## Do higher english scores get you higher salaries?\n",
    "## Is it after any particular score, that it starts to stop mattering?\n",
    "sns.set_style('darkgrid')\n",
    "plt.scatter(data.English, data.Salary)\n",
    "plt.title('English v/s Salary')\n",
    "plt.xlabel('English')\n",
    "plt.ylabel('Salary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similarly for Logical Ability\n",
    "# Can you fill in the missing code?\n",
    "plt.scatter(    ,   )\n",
    "plt.title('Logical v/s Salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Is Gender playing a role in deciding Salary? How do you visualise this relationship  between a category (Gender)\n",
    "## and Salary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.boxplot(by='Gender', column='Salary')\n",
    "plt.ylim(0,800000)\n",
    "## Note read line in median.\n",
    "## How to read this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############ Play area for session one ############\n",
    "## Why don't you plot a few scatters or hists etc for different variables?\n",
    "data[''].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(data[''], data[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Session 2  - Let's make our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT #4 - Modeling your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are interested in predicting Salary\n",
    "data.columns\n",
    "# For now, we will try to predict using only these columns:\n",
    "X_col = [u'Gender', u'10percentage', u'12graduation', u'12percentage', u'CollegeTier',\n",
    "     u'Degree', u'collegeGPA', u'CollegeCityTier', u'GraduationYear', u'English', u'Logical', \n",
    "     u'Quant', u'Domain', u'conscientiousness', u'agreeableness', u'extraversion', u'nueroticism', u'openess_to_experience']\n",
    "y_col = 'Salary'\n",
    "\n",
    "df_exp = df[X_col + [y_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Domain has a few missing values, many models can't handle missing values. For the sake of similicty we will use frame with\n",
    "## no missing values. We may lose some rows (the ones that have missing values)\n",
    "df_exp.dropna(subset=X_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  We lost a few rows, but it's okay. We could have let go of the domain column too, but we believe it could be important\n",
    "## in predicting the salary\n",
    "df_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_exp[X_col]\n",
    "y = df_exp.Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How does one row of X look?\n",
    "X.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Not all columns have numerical values. Not all models can handle categorical variables directly. For example, LinearRegression,\n",
    "# requires completely numerical input. So we will need to convert the categorical varibles to some numerical values.\n",
    "# Gender and Degree need our attention.\n",
    "\n",
    "# For Gender we will convert m --> 1 and f to --> 0 \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "X.loc[X.Gender == 'm', 'Gender'] = 1\n",
    "X.loc[X.Gender == 'f', 'Gender'] = 0\n",
    "\n",
    "print X.Gender.unique()\n",
    "\n",
    "# For Degree, which has 4 unique values, we will get 4 new columns representing each of the types of Degrees. If a candidate is\n",
    "# from any one of the degrees then that columns will have the value 1 (like ON) and the rest columns will be 0 (OFF).\n",
    "\n",
    "print 'Old shape', X.shape\n",
    "\n",
    "X = pd.get_dummies(X, columns=['Degree'])\n",
    "\n",
    "print 'New shape', X.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Check the last four new columns\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How does one row of X look now?\n",
    "X.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT #5 - Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "mdl = LinearRegression()\n",
    "mdl = mdl.fit(X, y)\n",
    "## thats it, you've trained your linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## These are the coefficients of the model\n",
    "mdl.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT #6 - Error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## But how does it do? \n",
    "explained_variance = mdl.score(X, y)\n",
    "print explained_variance\n",
    "## Note we are predicting the model on the data it  used to learn in the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## What score --> 0.15 mean? In sklearn regression models, scores are r^2 values. Which define the percentage of variange \n",
    "## explained. Root of the same number gives r, which is the correlation between the predicted and the observed values.\n",
    "correlation = np.sqrt(explained_variance)\n",
    "print correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## What are some other metrics for model eval?\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(y, mdl.predict(X))\n",
    "print 'Mean squared error is', mse\n",
    "print 'Root Mean squared error is', np.sqrt(mse)\n",
    "print 'Mean Absolute error is', mean_absolute_error(y, mdl.predict(X))\n",
    "## These give you a better intuitive sense of how off you are on an average in every prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## What exactly is this corerlation about?\n",
    "predicted_values = mdl.predict(X)\n",
    "plt.scatter(predicted_values, y)\n",
    "plt.title('Predicted v/s Observed')\n",
    "plt.plot([0,1000000],[0,1000000], c = 'black', alpha =0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## A little zoomed in?\n",
    "plt.scatter(predicted_values, y)\n",
    "plt.title('Predicted v/s Observed')\n",
    "plt.plot([0,1000000],[0,1000000], c = 'black', alpha =0.5)\n",
    "plt.xlim((0,1000000))\n",
    "plt.ylim((0,1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT #7 - DECISION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's use a decision tree. \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "## Note how each model in sklearn uses the same api ie. fit() and predict()\n",
    "mdl = DecisionTreeRegressor()\n",
    "mdl = mdl.fit(X, y)\n",
    "mdl.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_values = mdl.predict(X)\n",
    "plt.scatter(predicted_values, y)\n",
    "plt.title('Predicted v/s Observed')\n",
    "\n",
    "plt.plot([0,4000000],[0,4000000], c = 'black', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What? So the r is 1 !!\n",
    "# The decision tree was able to preict almost every possible values. Imagine a tree with\n",
    "#leaves for each data row. That's great! We have a model which predict salary with amazing accuracy. \n",
    "#But wait, we just trained the model on that data, it ought to know how to predict. But linearregression didn't give an amazing\n",
    "#accuracy even on the same data. \n",
    "# Our DT, had the flexiblity to train meld itself exactly as per the training samples, while the linear regression has the\n",
    "#constraint to be a line and so it did the best it could do as a line.\n",
    "#Nonetheless, we trained our model completely to the dataset. But our motive is to predict the grade for samples the model \n",
    "#doesn't know about, but does guesses/predicts based on his past learnings.\n",
    "# So what we need to do is test our models on unseen data. We will split our current dataset into 2 parts - train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Current dataset size', X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT 8 - TRAIN AND TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "## Get it? What happened? We made 4 sets.\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## lets train the dt on train set.\n",
    "#we train on the train set which is 3001 samples.\n",
    "mdl = DecisionTreeRegressor()\n",
    "mdl = mdl.fit(X_train, y_train)\n",
    "r2 = mdl.score(X_test, y_test)\n",
    "print 'On training data', mdl.score(X_train, y_train)\n",
    "print 'On unseen test data', r2\n",
    "# print np.sqrt(r2)\n",
    "\n",
    "mse = mean_squared_error(y_test, mdl.predict(X_test))\n",
    "print 'Mean squared error is', mse\n",
    "print 'Root Mean squared error is', np.sqrt(mse)\n",
    "print 'Mean Absolute error is', mean_absolute_error(y_test, mdl.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#See. That's how bad your model does. That's what you get for using a complicated/overfit model on train and expecting it\n",
    "# to work better on test. This was toh extreme example of how bad it does. Let's run a linear regression again. \n",
    "mdl = LinearRegression()\n",
    "mdl = mdl.fit(X_train, y_train)\n",
    "r2 = mdl.score(X_test, y_test)\n",
    "print 'On training data', mdl.score(X_train, y_train)\n",
    "print 'On unseen test data', r2\n",
    "\n",
    "\n",
    "## What are some other metrics for model eval?\n",
    "\n",
    "mse = mean_squared_error(y_test, mdl.predict(X_test))\n",
    "print 'Mean squared error is', mse\n",
    "print 'Root Mean squared error is', np.sqrt(mse)\n",
    "print 'Mean Absolute error is', mean_absolute_error(y_test, mdl.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Why do you think Linear Regression performed better than the DT and the performance didn't change much from the time we ran it on the whole dataset?\n",
    "\n",
    "Note: DT could have run better if we tweaked its params. Which we will see in the next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT #9 - OTHER ML TECHNIQUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######### Play area ########\n",
    "from sklearn.tree import ExtraTreeRegressor, DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "# Note: We've imported a few models, why don't you play around with them. Most of them have a tweakable parameters but you \n",
    "# can run them with their default values and see how they do. Report their r^2 values and other metrics. See how they change.\n",
    "# Train the models on X_train and predict on X_test. \n",
    "\n",
    "# All models in sklearn use the same api. model.fit(X, y) and then model.score(X, y) or model.predict(X)\n",
    "# You can do DecisionTreeRegressor? to know about the parameters it takes.\n",
    "\n",
    "# Uncomment the next line and run this cell. It's an example.\n",
    "# RandomForestRegressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session 3 - How to select the best parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "mdl = DecisionTreeRegressor()\n",
    "params = mdl.get_params()\n",
    "params\n",
    "## these are the params in a DT that are tweakable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_test = []\n",
    "results_train = []\n",
    "depths = range(1,40)\n",
    "for depth in depths:\n",
    "    mdl = DecisionTreeRegressor(max_depth=depth)\n",
    "    mdl = mdl.fit(X_train, y_train)\n",
    "    y_pred = mdl.predict(X_test)\n",
    "    results_test.append(np.sqrt(mean_squared_error(y_pred, y_test)))\n",
    "    \n",
    "    y_pred_train = mdl.predict(X_train)\n",
    "    results_train.append(np.sqrt(mean_squared_error(y_pred_train, y_train)))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(depths, results_test, label='Performance on Test')\n",
    "plt.plot(depths, results_train, c='red', label = 'Performance on Train')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('max_depth param in DT')\n",
    "plt.legend()\n",
    "plt.ylim((0,400000))\n",
    "plt.title('DT -  train and test rmse on range of max-depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DecisionTreeRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Max depth of three, longer the tree, the more it can overfit, but a too short a tree can't learn much.\n",
    "# vis a tree with each node taking a decision. Each node has one feature. So your max is 13 for our case.\n",
    "# How many samples atleast to have in each leaf, in the case we did overfit, each leaf had one sample, so we fitting so badly.\n",
    "# It's like for each example, the DT had a route. There was no generalization. The tree had rote learned each example. \n",
    "# Which turned out bad for us. \n",
    "# We are doing GridSearchCV. Groaan. So much type. \n",
    "# A grid search takes all the values we give it and runs a model on ver computation and tells us which does better.\n",
    "# It will run max_depth 3 with min_sample_leaf 1 and so on and so forth\n",
    "# But how does it evaluate which one is doing better? We learned how to do it using one test and train. So basically if I had\n",
    "# you to do it, you would run each model and see how it does on ^test^ and then use the hyperparam combo which does the best\n",
    "# on test for your model. Fair approach. But didn't we overfit on the test data then? And when real unseen data comes to the model,\n",
    "# it could end up doing bad? \n",
    "# Confusing?\n",
    "# Well, a normal test - train split works if you have a lot of data. A lot of data automatically generalizes quite a bit.\n",
    "# But in small data..problems. So..why not use another test and split into three? But then when will you stop?\n",
    "# Enter Cross Validation. You have 100 datapoints. You divide it into 5 folds. 100/5 = 20. So you have 5 sets of 20 points.\n",
    "# For each fold, you keep aside the 20, and train the model on 80 and test it on the 20 kept aside. You can do this 5 times, one\n",
    "# for each fold. Finally, you take the average of each score and you get a cv score for that experiement.\n",
    "# You do the same for each hyper parameter pair you have. And then you pick the one with the best cv score, resting assured\n",
    "# that the result is quite generalized.\n",
    "# So we are going to use GridSearchCV to do the same.\n",
    "# Giving it the values to run the model on. \n",
    "params = {'max_depth' : [3,4,5,6,7,8,9,10],\n",
    " 'min_samples_leaf':[10,20,30,40,50,100,200]\n",
    " }\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GridSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#init\n",
    "mdl_cv = GridSearchCV(mdl,params, cv=10 )\n",
    "\n",
    "# Here it does the magic\n",
    "# We don't have to test and train necessarily again, CV already helps generalize.\n",
    "# But just because we did it for regressor, we will do it. NOTE: We don't have to.\n",
    "mdl_cv = mdl_cv.fit(X_train, y_train)\n",
    "\n",
    "#This is what it selected\n",
    "mdl_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's see how it does on the test set. \n",
    "mdl_cv.score(X_test, y_test)\n",
    "\n",
    "## Go back up and see how DT did on test set without any parameter tuning! This is quite an improvement. But its still not\n",
    "## as good as a Linear Regression; which is okay; but you can try tweaking other parameters etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### PLaygroud #####\n",
    "# note: why don't you tweak parameters for other models such as the ones imported before?\n",
    "# get the parameters first, put values in the grid and then run GridSearch to find the best set of hyperparams.\n",
    "mdl = RandomForestRegressor()\n",
    "print mdl.get_params()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Choose parameters you want to tweak and give them values.\n",
    "params = {'n_estimators':[10,100,300], 'max_features':[5,6,7], 'min_samples_leaf':[1,10,20] }\n",
    "##that's a lot of combinations, this tuning is going to take time\n",
    "mdl_cv = GridSearchCV(mdl,params, cv=5)\n",
    "mdl_cv = mdl_cv.fit(X_train, y_train)\n",
    "mdl_cv.best_params_\n",
    "mdl_cv.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Feature selection\n",
    "from sklearn.feature_selection import f_regression, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape\n",
    "clf = SelectKBest(k=16).fit(X_train, y_train)\n",
    "X_train_k = clf.transform(X_train)\n",
    "X_test_k = clf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl = LinearRegression()\n",
    "mdl.fit(X_train_k, y_train)\n",
    "r2 = mdl.score(X_test_k, y_test)\n",
    "\n",
    "print 'On training data', mdl.score(X_train_k, y_train)\n",
    "print 'On unseen test data', r2\n",
    "# print np.sqrt(r2)\n",
    "\n",
    "mse = mean_squared_error(y_test, mdl.predict(X_test_k))\n",
    "print 'Mean squared error is', mse\n",
    "print 'Root Mean squared error is', np.sqrt(mse)\n",
    "print 'Mean Absolute error is', mean_absolute_error(y_test, mdl.predict(X_test_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Previous stats of regression with all features 21.\n",
    "\n",
    "# On training data 0.150187402806\n",
    "# On unseen test data 0.161858547543\n",
    "# Mean squared error is 35714037761.1\n",
    "# Root Mean squared error is 188981.580481\n",
    "# Mean Absolute error is 101768.379881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 16 feature mdl does bad with train data but slightly better with unseen data than the all-feature regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### play ground ####\n",
    "## Why don't you try playing around with some other methods such as PCA, ICA? \n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "## Use the same fit and transform paradigm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
